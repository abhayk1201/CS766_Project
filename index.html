<!DOCTYPE html>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
		<title>CS766 Project (LipGAN)</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="css_js_files/main.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">

		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body class="">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<div></div>
							<div></div>
							<div></div>
							<!-- Header -->
                            
                            <h1> LipGAN: Speech to Lip Sync Generation </h1>
                            	<h2><a href="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pptx"> (presentation slides) </a>
                            	<a href="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pdf"> (PDF) </a> </h2>

							<section>
							    <header class="major">
							        <h2>Motivation</h2>
							    </header>
								<div>
									<artical>
										While the manipulation of media is not a new phenomenon, highly realistic forms of synthetic media, i.e. realistic photo, audio, and video, are hitting the mainstream. A growing number of research has been conducted on this topic, including the synthesization of life-like talking face videos, also known as Audio Deepfakes. <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/facetoface_translation/paper.pdf"> Prajwal et al. </a> proposed a new approach in digital communication that was coined "Face-to-Face Translation”; by feeding in a static image or video of a person and a target audio file, one can produce talking face videos with natural lip and face synchronization. The idea behind this approach is to utilize a generative adversarial network (GAN) to generate face images conditioned on the audio input while also checking whether the generated frame and audio are synchronized. <br><br>
										Broad applications of this task include realistic foreign language dubbing in movies, CGI animation, conversational agents in telecommuting or distance education, and gaming. In this work, we explore this audio deepfake model, LipGAN, and attempt to improve upon its limitations.<br><br>
									</artical>
									<img src="files/lipgan_output_ex.gif" alt="">
									<figcaption>
										Fig.1: Example result obtained by LipGAN method. Model can accept still images or videos and a target audio file. Outputs a talking face video.
									</figcaption>
								</div>							 					
                             
				<br><br>
							</section>

                 				<section>
							<header class="major">
								<h2>Previous Work </h2>
							</header>
							<div>
								
								<h3>Image-to-image Translation</h3>
							        
					                  	<artical>
						 			Image-to-image translation has been a subject of reasearch for a long time. Previously, this was done by manual or semi-autmomatic methods. Some of the methods previously used include image smoothing and noise reduction techniques. Goodfellow et al introduced the use of  generative adversarial networks (GANs) to solve this problem in 2014.  <br><br>
									
					 			</artical>
								
								<h3>Automatic Speech Recognition</h3>
							     
								<artical>
									Automatic Speech Recognition is the process of converting speech into text. There have been many papers that looking into solving this problem. Deep Speech 1 uses a recurrent nerual network (RNN) that is trained on speech spectograms. This network converts the speech spectograms into character probabilities for the transcription. This network consists of five layers of hidden units with only the last two being recurrent. Nesterov's Accelerated gradient method is used for training. A N-gram language model is also integrated into this system. <br><br> 
									In Deep Speech 2 the network was expanded to be able to handle different languages and acents as well as noise. High performance computing is used to increase the speed 7x more than the previous model. The network in this model has up to 11 layers composed of bidirectional reccurent layers  and convolutional layers. A clipped ReLU is used as an activation function. <br><br> 
									
								</artical>
								
								<h3>Neural Machine Translation</h3>
							        
								<artical>
									Machine Translation is the process of converting text in one language to text in another language. Sutskever et al proposed solving this issues using neural networks in 2014. Since then furture improvements have been made by encoporating attention mechanisms. The most recent work by Vaswani et al. involves a transformer network which uses the attention mechanisms to draw global dependencies between input and output. <br><br>   
								</artical>
								
								<h3> Text to Speech </h3>
								<artical>
									Text to Speech is the process of converting text into speech (acoustic waveforms). A hidden Markov model (HMM) is a common model for this task. These models typically require less data but produce results that fail to capture prosody. <br><br> 
									Another approach that has been taken to this problem is to train a neural network.  Deep Voice 3 is a fully-convolutional attention-based neural text to speech model. This model consists of an encoder, decoder, and convertor.  
								</artical>
								
							        <h3> Voice Transfer in Audio </h3>
								<artical>
								Voice Transfer in Audio is the process of creating cross-language transfer of a synthetic voice to a natural speeker voice.  The CycleGan has acheived good results for this type of process. This model focuses on minimizing the differences in pixel values of natural voice and synthetic voice spectograms. 
								</artical>
				 			</div>
				

                             <br><br>
							</section>



							<section>
								<header class="major">
									<h2>Dataset and Preprocessing</h2>
								</header>
								<h3> Dataset Description </h3>
								We trained the model using the Lip Reading Senetences 2 (LRS2) [2], an audio-visual speech recognition dataset collected from in-the-wild videos. It consists of thousands of spoken sentences from BBC television. Each sentences is up to 100 characters in length. The training, validation, and test sets are divided according to broadcast date. The train set contains 45,839 utterances with 329,180 words instances and 17,660 vocab size.

								<br><br>
								<h3> Preprocessing </h3>
								In order to prepare the dataset, we extracted the frames at 25fps using opencv and trimmed the face coordinates of the speaker.  We used dlibget_frontal_face_detector to detect the speaker face from the video frames. Each detected face is resized to 96×96. LipGan requires that audio files are in the form of a Mel-frquency cepstrum features (MFCCs) so we converted the audio files to match this format. An example of preprocessed outputs for a video is shown in the figure below.
								<span>
									<img src="files/wecandobetter_preprocessing.png" alt="" width="100%">
									<figcaption>
										Fig.2: Pre-processeding output for the phrase "We can do better"
									</figcaption>

									<figcaption>Corresponding audio for the phrase:</figcaption>
									<br><br>
									<audio controls
										   src="files/audio.wav">
										Your browser does not support the
										<code>audio</code> element.
									</audio>

								</span>
							</section>
							


							<section>
							    <header class="major">
							        <h2>State-of-the-art Model Implementation </h2>
							    </header>
							    
							    <span>
                                 	<img src="files/mt_0_lip_GAN.png" alt="" width="100%">
                                	<figcaption>Fig.2 LipGAN architecture. <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/facetoface_translation/paper.pdf"> Paper Link </a>
                                	</figcaption>
							 	</span>						 	
							 	<br><br><br>

							 	<header class="major">
							 		<h2> Model architecture overview</h2>
							 	</header>

							 	<h2> Generator network </h2>

							 		<h3> Face Encoder </h3>
							 		This CNN module encodes the face features, including identity and pose.
							 		<br><br><br>

							 		<h3> Audio Encoder: </h3>
							 		The audio encoder is a standard CNN model that takes a Mel-frequency cepstral coefficient(MFCC) heatmap and creates an audio embedding
							 		<br><br><br>

							 		<h3> Face Decoder </h3>
							 		This module synthesizes a lip-synchronized face from the joint audio-visual embedding byinpainting the masked region of the input image with an appropriate mouth shape.
							 		<br><br><br>

							 	<h2> Discriminator network </h2>
							 	Contrastive loss between the encoded audio and encoded face is used to supervise the generator module to learn robust, accurate phoneme-viseme mappings to produce satisfactory talking faces with more natural facial movements.
							 	<br><br>

							</section>




  <!-- ########################################## Edit above this line; Below is ABHAY's edit ################################### -->

							<section>
							    <header class="major">
							        <h2>Experiments </h2>
							    </header>
							    <span> 
							    	<h3> Weighted L1 reconstruction loss </h3>
                                 	<img src="files/exp2.png" alt="" width="40%" align="right">
                                	<ul>
                                		<li> Idea: Lip reason contributes to less than 4% of the total reconstruction loss. Can we improve by focusing on the lip region. 
                                		<li> We observed that Lip region contributes to only 4% of total face ares i.e. the reconstruction loss. We hypothesize that we can use weighted loss to have extra supervision around lip region.                               
                                		<li>Early epochs vs late epochs reconstruction loss: Without weighted loss, the network starts to morph the lip region only at around half way through training process (roughly 10 epoch). Using weighted loss, even the earlier epochs start focusing on the lip region. But after 15 epochs or so, both models achieve similar L1 reconstruction loss (MAE).                                		                                			
                                		<li> One justification could be that we are passing the target pose prior (which is the face frame with masked lip region), Generative model learns to copy the non-lip face region anyways (be it in inital epochs or later epochs). Eventually focusing on lip region is redundant after midway through the training process, as both model versions will start focusing on the difficult part (morphing the lip region) than easy part (copy the non-lip region from target pose prior).
                                	</ul>


                                	<h3> Discriminator Network: Need for an expert discriminator </h3>
                                 	<img src="files/exp1.png" alt="" width="40%" align="right">
                                	<ul>
                                		<li> Idea: In the literature, researchers have tried using an additional expert lip sync discriminator. Note this expert discriminator is pre-trained and not updated during the training process. So, we wanted to establish the rationale behind how having an expert discriminator helps. 
                                		<li> We evaluated our dicriminator model (learned as a part of GAN framework), and found that it is indeed not a good discriminator and have just around 63% accuracy on 1000 randomly generated lip sync face images. 
                                		<li> The discriminator trained as a part of GAN framework is weak given the presenece of lot of artifacts due to large scale and pose variations. So, having an additional expert pre-trained discriminator provides better supervision to the generator network.
                                		<li> We wanted to try out multiple discriminators (one for visual quality, and one for lip sync) in a multi-task learning setting, but could not experiment given the resource limitation. (Implementational challenges discussed later)
                                	</ul>

							 	</span>
							 	<br>							 	
							</section>
							

							<section>
							    <header class="major">
							        <h2>Limitations </h2>
							    </header>
							    
							    <h3>Difficult  to  quantitatively measure  shortcomings</h3>
							    One of the biggest challenges that we faced was that it was hard to have a quantitative measure of the model’s performance which could substitute human qualitative evaluation. Most of the quantitative measures has its limitations, and we need to depend on human evaluation. For example, landmark distance which is defines as the sum of pointwise movement of lip keypoints over a time period.  The lower the better: This can be satisfied by just reducing the lip  movement globally (as in mumbling lip movement) . Similarly, other structural measure like SSIM (Structural SIMilarity Index) and PSNR (Peak signal-to-noise ratio) were designed to evaluate the overall image quality and not fine grained lip sync error.
  								<br><br><br>


							    <h3>Spurious lip region detection </h3>
							    We have noticed that lip-sync generation has spurious movements on non-lip region, like lower chin or side chin as shown below (right video). We have observed this when the face detection module fails to correctly localize the lip region. Profile view of the detected face usually faces this limitation. We are using dlib facial keypoints detector. 
							    <br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/biden2_out.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/biden_out.mp4" type="video/mp4">
									</video> 							 	
							 	
							 		<figcaption>Fig.3 From left to right: (a) Reasonably accurate lip sync </a> (b) Spurious lip sync on non-lip region  </a>
                                	</figcaption>
                                </span>
                                <br><br><br>


                                <h3>Profile face overcompensation/skewed lip sync</h3>
							    Profile view of the detected face usually suffers from skewed lip movements i.e One side of lip has more movement than the other side. 
							    <br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/AK_Nayak.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/profile.mp4" type="video/mp4">
									</video> 							 	
							 	
							 		<figcaption>Fig.4 From left to right: (a) Reasonably accurate lip sync </a> (b) Skewed lip sync for profile face view </a>
                                	</figcaption>
                                </span>
                                <br><br><br>



							
							    <h3>Teeth Region Deformation: crooked teeth OR no teeth at all</h3>
							    We observed that the LipGAN model generates image frames which smoothed out teeth and lip region. Lower teeth is merged with upper lip and smoothed out. In many cases, generated face images have crooked teeth, leading to very random/ discontinuous teeth contour. <br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/no_teeth.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/ak_side.mp4" type="video/mp4">
									</video> 							 	
							 	
								 	<figcaption>Fig.5 From left to right: (a) Generated lip-sync had missing teeth region </a> (b) Generated lip-synced frames with merged (smoothed) teeth and upper lip region </a>
	                                </figcaption>
                                </span>
                                <br><br><br>


                                <h3>Limitations due to facial expression </h3>
							    We also noticed that the model performs worse on faces with certain facial expressions, for example person in deep frown.
                                <br><br><br>

                                <h3>Issues with lip movement and audio synchronization</h3> 
                                Especially, background music leads to high murmuring lip movement. We usually don’t remove noise while using MFCC or Spectrogram heat maps, as usually learnt CNN filters should take care of that. But when we have background music (not noise) and music has a proper frequency representation, it becomes difficult for CNN models to distinguish music and speaker voice. Alternatively, we can remove noise during preprocessing step, but removing background music may require additional effort.
                                <span>
	                                <video width="460" height="315" controls>
									  <source src="files/trump_legacy_lip.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/noise.mp4" type="video/mp4">
									</video> 							 	
							 	
								 	<figcaption>Fig.6 From left to right: (a) Reasonably accurate lip sync </a> (b) Vibrating lips due to presence of noise or background music </a>
	                                </figcaption>
                                </span>
                                <br><br><br>

							    <h3>Implementation Challenges</h3>
							    We faced many implementational challenges- mostly due to the large dataset size and huge model trainable parameters.

							    <ul>
							    	<li> LRS2 dataset acquisition and pre-processing tasks were not easy given the huge size (roughly 50GB). Thepre-processed files were not readily available for use because of the signed Data Sharing agreement with BBC Research & Development. After several attempts, we were able to download the part files and save itto shared Google Drive. Now, we can mount the shared drive on Google Colab VM.
							    	<li> Another implementational challenge has to do with the limitations of Google Colab including running session timeouts and disk quota constraints.  Even having a google colab PRO subscription, it closes the job after a certain time (not fixed, usually 24ish hour). We had to be cautious not to exhaust google drive disk quota or File read-write operations. We trained our model on Google Colab.
							    	<li> Although  the  original  model  described  in  the  LipGAN  paper  utilizes  MATLAB  and  was  the  one  most extensively researched, due to issues working with the MATLAB implementation and our preference for using Python, we are using Python to implement the model. We had to resolve multiple python packages dependencies issues to have a working setup on GoogleColab.
							    </ul>
							</section>


							<section>
							    <header class="major">
							        <h2> Discussion </h2>
							    </header>							    								 	
							 	We have extensively discussed different categories of limitations in the above section. However, this this model is way more useful despite its shortcomings. Particularly, this is useful if we need to inpaint few frames in a larger video, for example missing frames in video due to internet network limitations. Another example is the use-case of a startup called "lyrebird.ai". Whenever we record, we tend to make some mistake that we want to modify later, but what if the error is in the middle of the video, we need to record it again otherwise stitching two different video segments may look abrupt. Lyrebird.ai solves that using lip syncing, basically you upload the video, it will generate the captioning, you just need to correct the captioning, and then it will OVER DUB the previous segment for the new corrected text; it generates your voice and lip syncing. 
							 	<br><br>
							 	We tried out the LipGAN model for cartoon characters too. It works reasonably well with animated cartoon as well. We can create lip-synced bitmoji or AR emoji using LipGAN.
							 	<br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/avatar.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/cartoon1.mp4" type="video/mp4">
									</video> 							 	
							 	
								 	<figcaption>Fig.7 From left to right: (a) avatar lip sync video </a> (b) cartoon lip sync video </a>
	                                </figcaption>
                                </span>
							</section>


							<section>
							    <header class="major">
							        <h2> Future Work </h2>
							    </header>	
							    Here are possible future work or points worth exploring: 
						    	<ul>
						    		<li> Need more detailed keypoint detector for lip region: Having more keypoints in lip region will help the model to create a more precise boundary between lips, teeth, side chin, etc. It will help in better supervision in reconstruction loss or discriminator network. 
						    		<li> Worth trying out 3D representation of face, with mesh-like grid to have more structured and smooth  lip movement along with side cheeks, jawline etc. (muscles getting pulled/pushed/squeezed). It will also enforce a sense of depth measure.
						    		<li> Million dollar idea: Live lip-syncing in video call, say zoom call (even if  speaker is not sharing his video feed, we just need one static face image). By doing so, we can have both live lip-synced video and privacy. Many psychological research have concluded that live video feed have better engagement. So we can use LipGAN generated anonymous/avatar video feed or using the static face image.
						    		<li> Lip-synced dubbed movies/Tv series: I lip-synced 10 min of “Money Heist” (dubbed from Spanish to English) and I definitely liked watching the lip-synced version!
						    	</ul>							 							
							</section>


							<section>
							    <header class="major">
							        <h2> Privacy Matters! </h2>
							    </header>	
							    Current  Deep Learning trends make it easier and easier to manipulate speech and video media. Even human evaluators can’t reliably tell if they’re real or fake. This is very impressive, but they also present a serious issue if they are in the wrong hands. As the saying goes- "with great power comes great responsibility".
							    This is just to RE-ITERATE that please be mindful of privacy if you do use similar models. 
                            
							</section>
							

							<section>
							    <header class="major">
							        <h2> Project Presentation Video </h2>
							    </header>							    	
							 	<span>
                                 <video width="900" height="600" controls>
								  <source src="files/class_presentation.mp4" type="video/mp4">
								</video> 
							 	</span>							 	
							</section>
							

							<section>
							    <header class="major">
							        <h2> Some additional Result samples </h2>
							    </header>
							    							   
							 	<span>
                                 <video width="460" height="400" controls>
								  <source src="files/abhay1.mp4" type="video/mp4">
								</video> 
							 	</span>

							 	<span>
							 	<video width="460" height="320" controls>
								  <source src="files/trump_srk.mp4" type="video/mp4">
								</video> 
							 	
							 	</span>
							</section>
				           
				           <section>
							    <header class="major">
							        <h2> PDF Reports </h2>
							    </header>
							   
							  <!--  	<iframe src="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pdf" frameborder="0" scrolling="yes"                           
                                    style="overflow: hidden; height: 100%; 
                                                width: 49%; float: left; " height="100%" width="49%"
                                   align="left">
                                  </iframe>  

 								<iframe src="files/CS766_ProjectProposal.pdf" frameborder="0" scrolling="yes"                           
                                    style="overflow: hidden; height: 100%; 
                                                width: 49%; float: left; " height="100%" width="49%"
                                   align="left">
                                  </iframe> -->
                                <span>
						           	
		    						<iframe src="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pdf" width="50%" height="400px" align="left">
		    						</iframe>
	    						
						           
		    						<iframe src="files/CS766_Project_midterm_report.pdf" width="50%" height="400px" align="right">
		    						</iframe>
	    						
						           	
		    						<iframe src="files/CS766_ProjectProposal.pdf" width="50%" height="400px" align="left">
		    						</iframe>
	    						
						           
		    						<iframe src="files/model_summary.pdf" width="50%" height="400px" align="right">
		    						</iframe>
		    						<br><br>
		    					</span>
	    						
	    					</section>


							<section>
							    <header class="major">
							        <h2> REFERENCES </h2>
							    </header>
							    							   
							 	<ul style="list-style-type:none;">
								  <li>[1] Prajwal KR, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, and CV Jawahar. Towards automatic face-to-face translation. In Proceedings of the 27th ACM International Conference on Multimedia, pages 1428–1436, 2019</li>
								  <br>
								  <li>[2] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Deep audio-visualspeech recognition. IEEE transactions on pattern analysis and machine intelligence, 2018.</li>
								  <br>
								  <li>[3] Lele Chen, Zhiheng Li, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. Lip movements generation at a glance. In Proceedings of the European Conference on Computer Vision (ECCV), pages 520–535, 2018.</li>
								  <br>
								  <li>[4] Joon Son Chung and Andrew Zisserman.  Out of time: automated lip sync in the wild.  In Asian conference oncomputer vision, pages 251–263. Springer, 2016.</li>
								  <br>
								  <li>[5] Lip  reading  sentences  in  the  wild  agreement  (lrs2)  document.https://www.bbc.co.uk/rd/projects/lip-reading-datasets.</li>
								  <br>
								  <li>[6] The oxford-bbc lip reading sentences 2 (lrs2) dataset.https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html.</li>
								  <br>
								  <li>[7]  Google colaboratory.https://colab.research.google.com/.</li>
								  <br>
								  <li>[8] https://github.com/Rudrabha/LipGAN/tree/fully_pythonic </li>
								  <br>
								  <li>[9] Prajwal, K.R., Mukhopadhyay, R., Namboodiri, V.P. and Jawahar, C.V., 2020, October. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia (pp. 484-492).</li>
								  <br>
								  <li>[10] Masood, Momina, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, and Aun Irtaza. "Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward." arXiv preprint arXiv:2103.00484 (2021).</li>
								  <br>
								  <li>[11] Prajwal, K.R., Mukhopadhyay, R., Namboodiri, V.P. and Jawahar, C.V., 2020. Learning individual speaking styles for accurate lip to speech synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 13796-13805).</li>
								  <br>
								  <li>[12] Chung, J.S., Jamaludin, A. and Zisserman, A., 2017. You said that?. arXiv preprint arXiv:1705.02966. </li>
								  <br>

								</ul>  
							</section>




						</div>
					</div>
					

					

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner" style="">


							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>CS766 Project (LipGAN)</h2>
									</header>
									<ul>
										<li><a href="proposal.html">Proposal</a></li>
										<li><a href="midterm.html">Mid-Term Report</a></li>
										<li><a href="index.html"><b>Final Report</b></a></li>
<!--										<li><a href="elements.html">Elements</a></li>-->
									</ul>
								</nav>


							<!-- Team -->
									<subheader class="major">
										<h2>Team Members</h2>
									</subheader>
									<ul>
										<li><a href="https://abhayk1201.github.io/">Abhay Kumar</a></li>
										<li><a href="https://github.com/maryamvaz">Maryam Vazirabad</a></li>
										<li><a href="">Elizabeth Murphy</a></li>
<!--										<li><a href="elements.html">Elements</a></li>-->
									</ul>
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">© Abhay Kumar, Maryam Vazirabd, Elizabeth Murphy&nbsp; Design: <a href="https://html5up.net/">HTML5 UP</a>.</p>
								</footer>

						</div>
					<a href="#sidebar" class="toggle">Toggle</a></div>


			</div>

		<!-- Scripts -->
			<script src="css_js_files/jquery.js"></script>
			<script src="css_js_files/skel.js"></script>
			<script src="css_js_files/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="css_js_files/main.js"></script>

	
</body></html>
