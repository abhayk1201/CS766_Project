<!DOCTYPE html>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
		<title>CS766 Project (LipGAN)</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="css_js_files/main.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">

		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body class="">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<div></div>
							<div></div>
							<div></div>
							<!-- Header -->
                            <h1>Final Report   <a href="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pptx"> (presentation slides) </a>
                            <a href="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pdf"> (PDF) </a>
                             </h1>
							<section>
							    <header class="major">
							        <h2>Introduction</h2>
							    </header>
							    <div>
							        <artical>
							             We explore the problem of lip-syncing a talking face video to match the target speech segment to the lip and facial expression of the person in the video.  The primary task is to achieve accurate audio-video 
							             synchronization given a person’s face and target audio clip, where videos feature faces are dynamic and unconstrained. <br><br>
							             This technique is broadly applicable to many scenarios such as realistic dubbing in the movie industry, conversational agents, virtual anchors, and gaming. Providing a natural lip movement and facial expression generation improves the user’s experience in these applications. <br><br> 
							             Despite the recent advances and its wide applicability, synthesizing a clear, accurate, and human-like performanceis still a challenging task.  We are exploring the limitations of the state-of-the-art techniques and propose possible solutions.<br><br>
							        </artical>
							    </div>							 					
                             <h3>Motivation and previous work </h3>
							</section>



							<section>
							    <header class="major">
							        <h2>Dataset and Preprocessing</h2>
							    </header>
							    <h3> Dataset Description </h3>
							    The LRS2 dataset [2] consists of thousands of spoken sentences from BBC television.  Each sentences is up to 100characters in length. The training, validation, and test sets are divided according to broadcast date. The train set contains45,839 utterances with 329,180 words instances and 17,660 vocab size.

							    <br><br>
							    <h3> Preprocessing </h3>
							    We extracted the frames at 25fps usingopencvand trimmed the face coordinates of the speaker.  We useddlibget_frontal_face_detectorto detect the speaker face from the video frames.  Each detected face is resized to96×96. The LipGan requires that audio files are in the form of a Mel-frquency cepstrum features (MFCCs) so weconverted the audio files using the pythonaudiolibrary to match this format.  Pre-processed outputs for a video is shown in below figure.
							    <span>
                                 	<img src="files/preprocessing.png" alt="" width="100%">
                                	<figcaption>Fig.1: Pre-processeding output for the utterance-"I Liked the radio podcast" 
                                	</figcaption>
							 	</span>						 					
							</section>
							


							<section>
							    <header class="major">
							        <h2>State-of-the-art Model Implementation </h2>
							    </header>
							    
							    <span>
                                 	<img src="files/mt_0_lip_GAN.png" alt="" width="100%">
                                	<figcaption>Fig.2 LipGAN architecture. <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/facetoface_translation/paper.pdf"> Paper Link </a>
                                	</figcaption>
							 	</span>
							 	
							 	<br>
							 	<h2> Model architecture overview: Generator network </h2>


							 		<h3> Face Encoder </h3>

							 		This CNN module encodes the face features, including identity and pose.

							 		<h3> Audio Encoder: </h3>
							 		The audio encoder is a standard CNN model that takes a Mel-frequency cepstral coefficient(MFCC) heatmap and creates an audio embedding

							 		<h3> Face Decoder </h3>
							 		This module synthesizes a lip-synchronized face from the joint audio-visual embedding byinpainting the masked region of the input image with an appropriate mouth shape.


							 	<h2> Discriminator network </h2>
							 	Contrastive loss between the encoded audio and encoded face is used to supervise the generator module to learn robust,accurate phoneme-viseme mappings to produce satisfactory talking faces with more natural facial movements.
							 	<br><br>

							</section>




  <!-- ########################################## Edit above this line; Below is ABHAY's edit ################################### -->

							<section>
							    <header class="major">
							        <h2>Experiments </h2>
							    </header>
							    <span> 
							    	<h3> Weighted L1 reconstruction loss </h3>
                                 	<img src="files/exp2.png" alt="" width="40%" align="right">
                                	<ul>
                                		<li> Idea: Lip reason contributes to less than 4% of the total reconstruction loss. Can we improve by focusing on the lip region. 
                                		<li> We observed that Lip region contributes to only 4% of total face ares i.e. the reconstruction loss. We hypothesize that we can use weighted loss to have extra supervision around lip region.                               
                                		<li>Early epochs vs late epochs reconstruction loss: Without weighted loss, the network starts to morph the lip region only at around half way through training process (roughly 10 epoch). Using weighted loss, even the earlier epochs start focusing on the lip region. But after 15 epochs or so, both models achieve similar L1 reconstruction loss (MAE).                                		                                			
                                		<li> One justification could be that we are passing the target pose prior (which is the face frame with masked lip region), Generative model learns to copy the non-lip face region anyways (be it in inital epochs or later epochs). Eventually focusing on lip region is redundant after midway through the training process, as both model versions will start focusing on the difficult part (morphing the lip region) than easy part (copy the non-lip region from target pose prior).
                                	</ul>


                                	<h3> Discriminator Network: Need for an expert discriminator </h3>
                                 	<img src="files/exp1.png" alt="" width="40%" align="right">
                                	<ul>
                                		<li> Idea: In the literature, researchers have tried using an additional expert lip sync discriminator. Note this expert discriminator is pre-trained and not updated during the training process. So, we wanted to establish the rationale behind how having an expert discriminator helps. 
                                		<li> We evaluated our dicriminator model (learned as a part of GAN framework), and found that it is indeed not a good discriminator and have just around 63% accuracy on 1000 randomly generated lip sync face images. 
                                		<li> The discriminator trained as a part of GAN framework is weak given the presenece of lot of artifacts due to large scale and pose variations. So, having an additional expert pre-trained discriminator provides better supervision to the generator network.
                                		<li> We wanted to try out multiple discriminators (one for visual quality, and one for lip sync) in a multi-task learning setting, but could not experiment given the resource limitation. (Implementational challenges discussed later)
                                	</ul>

							 	</span>
							 	<br>							 	
							</section>
							

							<section>
							    <header class="major">
							        <h2>Limitations </h2>
							    </header>
							    
							    <h3>Difficult  to  quantitatively measure  shortcomings</h3>
							    One of the biggest challenges that we faced was that it was hard to have a quantitative measure of the model’s performance which could substitute human qualitative evaluation. Most of the quantitative measures has its limitations, and we need to depend on human evaluation. For example, landmark distance which is defines as the sum of pointwise movement of lip keypoints over a time period.  The lower the better: This can be satisfied by just reducing the lip  movement globally (as in mumbling lip movement) . Similarly, other structural measure like SSIM (Structural SIMilarity Index) and PSNR (Peak signal-to-noise ratio) were designed to evaluate the overall image quality and not fine grained lip sync error.
  								<br><br><br>


							    <h3>Spurious lip region detection </h3>
							    We have noticed that lip-sync generation has spurious movements on non-lip region, like lower chin or side chin as shown below (right video). We have observed this when the face detection module fails to correctly localize the lip region. Profile view of the detected face usually faces this limitation. We are using dlib facial keypoints detector. 
							    <br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/biden2_out.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/biden_out.mp4" type="video/mp4">
									</video> 							 	
							 	
							 		<figcaption>Fig.3 From left to right: (a) Reasonably accurate lip sync </a> (b) Spurious lip sync on non-lip region  </a>
                                	</figcaption>
                                </span>
                                <br><br><br>


                                <h3>Profile face overcompensation/skewed lip sync</h3>
							    Profile view of the detected face usually suffers from skewed lip movements i.e One side of lip has more movement than the other side. 
							    <br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/AK_Nayak.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/profile.mp4" type="video/mp4">
									</video> 							 	
							 	
							 		<figcaption>Fig.4 From left to right: (a) Reasonably accurate lip sync </a> (b) Skewed lip sync for profile face view </a>
                                	</figcaption>
                                </span>
                                <br><br><br>



							
							    <h3>Teeth Region Deformation: crooked teeth OR no teeth at all</h3>
							    We observed that the LipGAN model generates image frames which smoothed out teeth and lip region. Lower teeth is merged with upper lip and smoothed out. In many cases, generated face images have crooked teeth, leading to very random/ discontinuous teeth contour. <br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/no_teeth.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/ak_side.mp4" type="video/mp4">
									</video> 							 	
							 	
								 	<figcaption>Fig.5 From left to right: (a) Generated lip-sync had missing teeth region </a> (b) Generated lip-synced frames with merged (smoothed) teeth and upper lip region </a>
	                                </figcaption>
                                </span>
                                <br><br><br>


                                <h3>Limitations due to facial expression </h3>
							    We also noticed that the model performs worse on faces with certain facial expressions, for example person in deep frown.
                                <br><br><br>

                                <h3>Issues with lip movement and audio synchronization</h3> 
                                Especially, background music leads to high murmuring lip movement. We usually don’t remove noise while using MFCC or Spectrogram heat maps, as usually learnt CNN filters should take care of that. But when we have background music (not noise) and music has a proper frequency representation, it becomes difficult for CNN models to distinguish music and speaker voice. Alternatively, we can remove noise during preprocessing step, but removing background music may require additional effort.
                                <span>
	                                <video width="460" height="315" controls>
									  <source src="files/trump_legacy_lip.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/noise.mp4" type="video/mp4">
									</video> 							 	
							 	
								 	<figcaption>Fig.6 From left to right: (a) Reasonably accurate lip sync </a> (b) Vibrating lips due to presence of noise or background music </a>
	                                </figcaption>
                                </span>
                                <br><br><br>

							    <h3>Implementation Challenges</h3>
							    We faced many implementational challenges- mostly due to the large dataset size and huge model trainable parameters.

							    <ul>
							    	<li> LRS2 dataset acquisition and pre-processing tasks were not easy given the huge size (roughly 50GB). Thepre-processed files were not readily available for use because of the signed Data Sharing agreement with BBC Research & Development. After several attempts, we were able to download the part files and save itto shared Google Drive. Now, we can mount the shared drive on Google Colab VM.
							    	<li> Another implementational challenge has to do with the limitations of Google Colab including running session timeouts and disk quota constraints.  Even having a google colab PRO subscription, it closes the job after a certain time (not fixed, usually 24ish hour). We had to be cautious not to exhaust google drive disk quota or File read-write operations. We trained our model on Google Colab.
							    	<li> Although  the  original  model  described  in  the  LipGAN  paper  utilizes  MATLAB  and  was  the  one  most extensively researched, due to issues working with the MATLAB implementation and our preference for using Python, we are using Python to implement the model. We had to resolve multiple python packages dependencies issues to have a working setup on GoogleColab.
							    </ul>
							</section>


							<section>
							    <header class="major">
							        <h2> Discussion </h2>
							    </header>							    								 	
							 	We have extensively discussed different categories of limitations in the above section. However, this this model is way more useful despite its shortcomings. Particularly, this is useful if we need to inpaint few frames in a larger video, for example missing frames in video due to internet network limitations. Another example is the use-case of a startup called "lyrebird.ai". Whenever we record, we tend to make some mistake that we want to modify later, but what if the error is in the middle of the video, we need to record it again otherwise stitching two different video segments may look abrupt. Lyrebird.ai solves that using lip syncing, basically you upload the video, it will generate the captioning, you just need to correct the captioning, and then it will OVER DUB the previous segment for the new corrected text; it generates your voice and lip syncing. 
							 	<br><br>
							 	We tried out the LipGAN model for cartoon characters too. It works reasonably well with animated cartoon as well. We can create lip-synced bitmoji or AR emoji using LipGAN.
							 	<br><br>
							 	<span>
	                                <video width="460" height="315" controls>
									  <source src="files/avatar.mp4" type="video/mp4">
									</video> 

								 	<video width="460" height="320" controls>
									  <source src="files/cartoon1.mp4" type="video/mp4">
									</video> 							 	
							 	
								 	<figcaption>Fig.7 From left to right: (a) avatar lip sync video </a> (b) cartoon lip sync video </a>
	                                </figcaption>
                                </span>
							</section>


							<section>
							    <header class="major">
							        <h2> Future Work </h2>
							    </header>	
							    Here are possible future work or points worth exploring: 
						    	<ul>
						    		<li> Need more detailed keypoint detector for lip region: Having more keypoints in lip region will help the model to create a more precise boundary between lips, teeth, side chin, etc. It will help in better supervision in reconstruction loss or discriminator network. 
						    		<li> Worth trying out 3D representation of face, with mesh-like grid to have more structured and smooth  lip movement along with side cheeks, jawline etc. (muscles getting pulled/pushed/squeezed). It will also enforce a sense of depth measure.
						    		<li> Million dollar idea: Live lip-syncing in video call, say zoom call (even if  speaker is not sharing his video feed, we just need one static face image). By doing so, we can have both live lip-synced video and privacy. Many psychological research have concluded that live video feed have better engagement. So we can use LipGAN generated anonymous/avatar video feed or using the static face image.
						    		<li> Lip-synced dubbed movies/Tv series: I lip-synced 10 min of “Money Heist” (dubbed from Spanish to English) and I definitely liked watching the lip-synced version!
						    	</ul>							 							
							</section>


							<section>
							    <header class="major">
							        <h2> Privacy Matters! </h2>
							    </header>	
							    Current  Deep Learning trends make it easier and easier to manipulate speech and video media. Even human evaluators can’t reliably tell if they’re real or fake. This is very impressive, but they also present a serious issue if they are in the wrong hands. As the saying goes- "with great power comes great responsibility".
							    This is just to RE-ITERATE that please be mindful of privacy if you do use similar models. 
                            
							</section>
							

							<section>
							    <header class="major">
							        <h2> Project Presentation Video </h2>
							    </header>							    	
							 	<span>
                                 <video width="900" height="600" controls>
								  <source src="files/class_presentation.mp4" type="video/mp4">
								</video> 
							 	</span>							 	
							</section>
							

							<section>
							    <header class="major">
							        <h2> Some additional Result samples </h2>
							    </header>
							    							   
							 	<span>
                                 <video width="460" height="400" controls>
								  <source src="files/abhay1.mp4" type="video/mp4">
								</video> 
							 	</span>

							 	<span>
							 	<video width="460" height="320" controls>
								  <source src="files/trump_srk.mp4" type="video/mp4">
								</video> 
							 	
							 	</span>
							</section>
				           
				           <section>
							    <header class="major">
							        <h2> PDF Reports </h2>
							    </header>
							   
							  <!--  	<iframe src="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pdf" frameborder="0" scrolling="yes"                           
                                    style="overflow: hidden; height: 100%; 
                                                width: 49%; float: left; " height="100%" width="49%"
                                   align="left">
                                  </iframe>  

 								<iframe src="files/CS766_ProjectProposal.pdf" frameborder="0" scrolling="yes"                           
                                    style="overflow: hidden; height: 100%; 
                                                width: 49%; float: left; " height="100%" width="49%"
                                   align="left">
                                  </iframe> -->
                                <span>
						           	
		    						<iframe src="files/CS766-LIP-GAN_SPEECH_TO_LIP_SYNC_GENERATION.pdf" width="50%" height="400px" align="left">
		    						</iframe>
	    						
						           
		    						<iframe src="files/CS766_Project_Midterm_report.pdf" width="50%" height="400px" align="right">
		    						</iframe>
	    						
						           	
		    						<iframe src="files/CS766_ProjectProposal.pdf" width="50%" height="400px" align="left">
		    						</iframe>
	    						
						           
		    						<iframe src="model_summary.pdf" width="50%" height="400px" align="right">
		    						</iframe>
		    						<br><br>
		    					</span>
	    						
	    					</section>


							<section>
							    <header class="major">
							        <h2> REFERENCES </h2>
							    </header>
							    							   
							 	<ul style="list-style-type:none;">
								  <li>[1] Prajwal KR, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, and CV Jawahar. Towards automatic face-to-face translation. In Proceedings of the 27th ACM International Conference on Multimedia, pages 1428–1436, 2019</li>
								  <br>
								  <li>[2] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Deep audio-visualspeech recognition. IEEE transactions on pattern analysis and machine intelligence, 2018.</li>
								  <br>
								  <li>[3] Lele Chen, Zhiheng Li, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. Lip movements generation at a glance. In Proceedings of the European Conference on Computer Vision (ECCV), pages 520–535, 2018.</li>
								  <br>
								  <li>[4] Joon Son Chung and Andrew Zisserman.  Out of time: automated lip sync in the wild.  In Asian conference oncomputer vision, pages 251–263. Springer, 2016.</li>
								  <br>
								  <li>[5] Lip  reading  sentences  in  the  wild  agreement  (lrs2)  document.https://www.bbc.co.uk/rd/projects/lip-reading-datasets.</li>
								  <br>
								  <li>[6] The oxford-bbc lip reading sentences 2 (lrs2) dataset.https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html.</li>
								  <br>
								  <li>[7]  Google colaboratory.https://colab.research.google.com/.</li>
								  <br>
								  <li>[8] https://github.com/Rudrabha/LipGAN/tree/fully_pythonic </li>
								  <br>
								  <li>[9] Prajwal, K.R., Mukhopadhyay, R., Namboodiri, V.P. and Jawahar, C.V., 2020, October. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia (pp. 484-492).</li>
								  <br>
								  <li>[10] Masood, Momina, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, and Aun Irtaza. "Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward." arXiv preprint arXiv:2103.00484 (2021).</li>
								  <br>
								  <li>[11] Prajwal, K.R., Mukhopadhyay, R., Namboodiri, V.P. and Jawahar, C.V., 2020. Learning individual speaking styles for accurate lip to speech synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 13796-13805).</li>
								  <br>
								  <li>[12] Chung, J.S., Jamaludin, A. and Zisserman, A., 2017. You said that?. arXiv preprint arXiv:1705.02966. </li>
								  <br>

								</ul>  
							</section>




						</div>
					</div>
					

					

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner" style="">


							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>CS766 Project (LipGAN)</h2>
									</header>
									<ul>
										<li><a href="proposal.html">Proposal</a></li>
										<li><a href="midterm.html">Mid-Term Report</a></li>
										<li><a href="index.html"><b>Final Report</b></a></li>
<!--										<li><a href="elements.html">Elements</a></li>-->
									</ul>
								</nav>


							<!-- Team -->
									<subheader class="major">
										<h2>Team Members</h2>
									</subheader>
									<ul>
										<li><a href="https://abhayk1201.github.io/">Abhay Kumar</a></li>
										<li><a href="">Maryam Vazirabd</a></li>
										<li><a href="">Elizabeth Murphy</a></li>
<!--										<li><a href="elements.html">Elements</a></li>-->
									</ul>
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">© Abhay Kumar, Maryam Vazirabd, Elizabeth Murphy&nbsp; Design: <a href="https://html5up.net/">HTML5 UP</a>.</p>
								</footer>

						</div>
					<a href="#sidebar" class="toggle">Toggle</a></div>


			</div>

		<!-- Scripts -->
			<script src="css_js_files/jquery.js"></script>
			<script src="css_js_files/skel.js"></script>
			<script src="css_js_files/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="css_js_files/main.js"></script>

	
</body></html>