<!DOCTYPE html>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
		<title>CS766 Project (LipGAN)</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="css_js_files/main.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">

		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body class="">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<div></div>
							<div></div>
							<div></div>
							<!-- Header -->
                            <h1>Mid-Term Report   <a href="files/CS766_Project_midterm_report.pdf"> (PDF) </a> </h1>
							<section>
							    <header class="major">
							        <h2>Introduction</h2>
							    </header>
							    <div>
							        <artical>
							             We explore the problem of lip-syncing a talking face video to match the target speech segment to the lip and facial expression of the person in the video.  The primary task is to achieve accurate audio-video 
							             synchronization given a person’s face and target audio clip, where videos feature faces are dynamic and unconstrained. <br><br>
							             This technique is broadly applicable to many scenarios such as realistic dubbing in the movie industry, conversational agents, virtual anchors, and gaming. Providing a natural lip movement and facial expression generation improves the user’s experience in these applications. <br><br> 
							             Despite the recent advances and its wide applicability, synthesizing a clear, accurate, and human-like performanceis still a challenging task.  We are exploring the limitations of the state-of-the-art techniques and propose possible solutions.<br><br>

										 	
							        </artical>
							    </div>

							 
							 

<!-- 							 <span><img src="css_js_files/proposal_scale.png" alt="" width="30%"></span>

							 <span><img src="css_js_files/proposal_crop.png" alt="" width="30%"></span>

							 <span><img src="css_js_files/proposal_sc.png" alt="" width="30%"></span>
                             <figcaption>Fig.2. From left to right, Result of scaling, Cropping, Seam-carving</figcaption> -->

							</section>
							
							<section>
							    <header class="major">
							        <h2>State-of-the-art Model Implementation </h2>
							    </header>
							    <span>
                                 <img src="files/mt_0_lip_GAN.png" alt="" width="100%">
                                <figcaption>Fig.1 LipGAN architecture. <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/facetoface_translation/paper.pdf"> Paper Link </a>
                                </figcaption>
							 	</span>
							 	<br>
							 	
							</section>
							

							<section>
							    <header class="major">
							        <h2>Problems Faced & Possible directions </h2>
							    </header>
							    
							    <h3>Limitations due to spurious lip region detection: </h3>
							    We have noticed that lip-sync generation has spuriousmovements on non-lip region, like lower chin or side chin as shown below (right video). We have observed this when the face detection fails to correctly localize the lip region. Profile view of the detected face usually faces this limitation. We need to observe few more such cases to generalize the limitation and come up with a possible modification.  <br><br>
							 	<span>
                                 <video width="460" height="315" controls>
								  <source src="files/biden2_out.mp4" type="video/mp4">
								</video> 
							 	</span>
							 	<!-- <br><br>
							 	<h3> LipGAN Paper Video Demo
							 	</h3> -->
							 	<span>

							 	<video width="460" height="320" controls>
								  <source src="files/biden_out.mp4" type="video/mp4">
								</video> 
							 	
							 	</span>
							 	 <figcaption>Fig.2 From left to right: (a) LipGAN generates reasonably accurate lip sync </a> (b) LipGAN generates spurious lip sync on non-lip region  </a>
                                </figcaption>
							</section>


							<section>
							    <h3>Teeth Region Deformation</h3>
							    We observed that the LipGAN model generates image frames which smoothed out teeth and lip region. Lower teeth is merged with upper lip and smoothed out. <br><br>
							 	<span>
                                 <video width="460" height="315" controls>
								  <source src="files/AK_Nayak.mp4" type="video/mp4">
								</video> 
							 	</span>
							 	<!-- <br><br>
							 	<h3> LipGAN Paper Video Demo
							 	</h3> -->
							 	<span>

							 	<video width="460" height="320" controls>
								  <source src="files/ak_side.mp4" type="video/mp4">
								</video> 
							 	
							 	</span>
							 	 <figcaption>Fig.3 From left to right: (a) LipGAN generates reasonably accurate lip sync </a> (b) LipGAN generates frames with merged (smoothed) teeth and upper lip region </a>
                                </figcaption>
							</section>


							<section>
							    <h3>Ongoing Progress</h3>
							    We are trying to modify the face reconstruction loss module. Currently, the face reconstruction loss is calculated for the whole image to ensure correct pose generation, background around the face, and preservation of the identity.  The lip region contributes to less than 4% of the total reconstruction loss. However, we need to emphasize the reconstruction loss in lip region. We are planning to explore different techniques, like weighted reconstruction loss, or having a separate discriminator (as in a multi-task setting) to focus on the lip-sync only. We can jointly train the GAN framework with two discriminator networks (one for visual quality, and one for lip sync). Additionally, We will manually evaluate and judge the lip-synchronization based on performance metrics discussed in previous works. <br><br>
                            	
                            	<h3>Optional Goal</h3>
							    If time and computational power permit, we can experiment with different model architectures for each of the blocks shown in the LipGAN architecture. For example, we can use state-of-the-art model architectures to extract richer and complex audio and face embedding. <br><br>
                            
							</section>
							

							<section>
							    <header class="major">
							        <h2> Some additional Result samples </h2>
							    </header>
							    
							   
							 	<span>
                                 <video width="460" height="400" controls>
								  <source src="files/abhay1.mp4" type="video/mp4">
								</video> 
							 	</span>
							 	<!-- <br><br>
							 	<h3> LipGAN Paper Video Demo
							 	</h3> -->
							 	<span>

							 	<video width="460" height="320" controls>
								  <source src="files/trump_srk.mp4" type="video/mp4">
								</video> 
							 	
							 	</span>
							</section>
				           
						</div>
					</div>
					

					

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner" style="">


							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>CS766 Project (LipGAN)</h2>
									</header>
									<ul>
										<li><a href="index.html">Proposal</a></li>
										<li><a href="midterm.html"><b>Mid-Term Report</b></a></li>
										<li><a href="final.html">Final Report</a></li>
<!--										<li><a href="elements.html">Elements</a></li>-->
									</ul>
								</nav>


							<!-- Team -->
									<subheader class="major">
										<h2>Team Members</h2>
									</subheader>
									<ul>
										<li><a href="https://abhayk1201.github.io/">Abhay Kumar</a></li>
										<li><a href="">Maryam Vazirabd</a></li>
										<li><a href="">Elizabeth Murphy</a></li>
<!--										<li><a href="elements.html">Elements</a></li>-->
									</ul>
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">© Abhay Kumar, Maryam Vazirabd, Elizabeth Murphy&nbsp; Design: <a href="https://html5up.net/">HTML5 UP</a>.</p>
								</footer>

						</div>
					<a href="#sidebar" class="toggle">Toggle</a></div>


			</div>

		<!-- Scripts -->
			<script src="css_js_files/jquery.js"></script>
			<script src="css_js_files/skel.js"></script>
			<script src="css_js_files/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="css_js_files/main.js"></script>

	
</body></html>